"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[95],{1892:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Introduction-to-Physical-AI/What-is-Physical-AI","title":"What is Physical AI?","description":"Overview","source":"@site/docs/Introduction-to-Physical-AI/01-What-is-Physical-AI.md","sourceDirName":"Introduction-to-Physical-AI","slug":"/Introduction-to-Physical-AI/What-is-Physical-AI","permalink":"/textbook.github.io/docs/Introduction-to-Physical-AI/What-is-Physical-AI","draft":false,"unlisted":false,"editUrl":"https://github.com/Mohammed-Umair-Kalim/textbook.github.io/docs/Introduction-to-Physical-AI/01-What-is-Physical-AI.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter Plan: Introduction to Physical AI","permalink":"/textbook.github.io/docs/Introduction-to-Physical-AI/plan"},"next":{"title":"A Brief History of AI and Robotics","permalink":"/textbook.github.io/docs/Introduction-to-Physical-AI/A-Brief-History-of-AI-and-Robotics"}}');var o=i(4848),s=i(8453);const a={sidebar_position:1},r="What is Physical AI?",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Detailed Explanation",id:"detailed-explanation",level:2},{value:"The Crisis of Traditional AI: The &quot;Brain in a Vat&quot;",id:"the-crisis-of-traditional-ai-the-brain-in-a-vat",level:3},{value:"The Embodied Revolution",id:"the-embodied-revolution",level:3},{value:"Core Concepts of Physical AI",id:"core-concepts-of-physical-ai",level:3},{value:"References",id:"references",level:3},{value:"Hands-on Exercise: Simulating a Basic Perception-Action Loop in ROS2",id:"hands-on-exercise-simulating-a-basic-perception-action-loop-in-ros2",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Create a ROS2 Package",id:"step-1-create-a-ros2-package",level:3},{value:"Step 2: Implement the Perception Node",id:"step-2-implement-the-perception-node",level:3},{value:"Step 3: Implement the Action Node",id:"step-3-implement-the-action-node",level:3},{value:"Step 4: Build and Run",id:"step-4-build-and-run",level:3},{value:"Case Study: Shakey the Robot - The First &quot;Person&quot;",id:"case-study-shakey-the-robot---the-first-person",level:2},{value:"References",id:"references-1",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"what-is-physical-ai",children:"What is Physical AI?"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:["This section defines Physical AI, a subfield of artificial intelligence that emphasizes the crucial role of a physical body in developing intelligence. Unlike traditional AI, which often exists in simulated or purely digital environments, Physical AI is concerned with intelligent agents that are ",(0,o.jsx)(n.em,{children:"embodied"}),", ",(0,o.jsx)(n.em,{children:"situated"})," in the real world, and learn through ",(0,o.jsx)(n.em,{children:"interaction"}),'. We will explore these core concepts, drawing a clear line between the disembodied "brain in a vat" and an intelligence that is fundamentally shaped by its physical form and environment.']}),"\n",(0,o.jsx)(n.h2,{id:"detailed-explanation",children:"Detailed Explanation"}),"\n",(0,o.jsx)(n.h3,{id:"the-crisis-of-traditional-ai-the-brain-in-a-vat",children:'The Crisis of Traditional AI: The "Brain in a Vat"'}),"\n",(0,o.jsx)(n.p,{children:'For much of its history, AI research focused on abstract reasoning and problem-solving, detached from physical reality. This "disembodied" approach, heavily influenced by the symbolic processing paradigm of pioneers like Newell and Simon (1956), treated intelligence as a matter of manipulating symbols in a formal system. While this led to impressive feats in games like chess, it struggled to address the complexities of the real world. This limitation became known as the "frame problem"\u2014the challenge for a purely logical system to determine which facts about the world change and which remain the same after an action is performed. The world is simply too complex and dynamic to be fully captured in a set of predefined symbolic representations.'}),"\n",(0,o.jsx)(n.h3,{id:"the-embodied-revolution",children:"The Embodied Revolution"}),"\n",(0,o.jsx)(n.p,{children:'In the late 1980s and early 1990s, a new paradigm emerged, championed by roboticists like Rodney Brooks. The central idea was "intelligence without representation" (Brooks, 1991). This approach argued that intelligent behavior could emerge from simple, direct interactions with the environment, without the need for complex internal models of the world. This marked the beginning of the embodied AI revolution.'}),"\n",(0,o.jsx)(n.h3,{id:"core-concepts-of-physical-ai",children:"Core Concepts of Physical AI"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"1. Embodiment:"})," At its heart, Physical AI posits that the body is not just a passive container for a brain, but an active participant in the cognitive process. The physical form of an agent\u2014its sensors, actuators, and morphology\u2014constrains and shapes its intelligence. For example, the way a humanoid robot with two legs perceives and navigates the world is fundamentally different from how a snake-like robot does. The body is not a mere output device; it is an integral part of the thinking process (Pfeifer & Bongard, 2006)."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"2. Situatedness:"})," Physical AI agents are ",(0,o.jsx)(n.em,{children:"situated"}),' or "embedded" in their environment. They are not detached observers but are directly coupled with the world around them. This means they are subject to the messiness and unpredictability of reality: incomplete information, sensor noise, and unexpected obstacles. An agent\'s intelligence is measured by its ability to cope with these challenges in real-time.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"3. Interaction:"})," Intelligence in Physical AI is developed and demonstrated through ",(0,o.jsx)(n.em,{children:"interaction"}),". It is the continuous dialogue between an agent's actions and the environment's feedback that drives learning and adaptation. This is formalized in the ",(0,o.jsx)(n.strong,{children:"Perception-Action Loop"}),", which we will explore in the next section. Through this loop, the agent learns to associate its actions with outcomes, gradually building a repertoire of skilled behaviors."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Brooks, R. A. (1991). ",(0,o.jsx)(n.em,{children:"Intelligence without representation"}),". Artificial intelligence, 47(1-3), 139-159."]}),"\n",(0,o.jsxs)(n.li,{children:["Newell, A., & Simon, H. A. (1956). ",(0,o.jsx)(n.em,{children:"The logic theory machine\u2014A complex information processing system"}),". IRE Transactions on information theory, 2(3), 61-79."]}),"\n",(0,o.jsxs)(n.li,{children:["Pfeifer, R., & Bongard, J. (2006). ",(0,o.jsx)(n.em,{children:"How the body shapes the way we think: a new view of intelligence"}),". MIT press."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-exercise-simulating-a-basic-perception-action-loop-in-ros2",children:"Hands-on Exercise: Simulating a Basic Perception-Action Loop in ROS2"}),"\n",(0,o.jsx)(n.p,{children:"This exercise will guide you through creating a simplified Perception-Action loop using ROS2 and Python. We will create two nodes:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["A ",(0,o.jsx)(n.code,{children:"perception_node"})," that simulates a sensor detecting an object."]}),"\n",(0,o.jsxs)(n.li,{children:["An ",(0,o.jsx)(n.code,{children:"action_node"}),' that "acts" based on the sensor data.']}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A working ROS2 installation (Humble Hawksbill recommended)."}),"\n",(0,o.jsx)(n.li,{children:"A ROS2 workspace created."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-1-create-a-ros2-package",children:"Step 1: Create a ROS2 Package"}),"\n",(0,o.jsxs)(n.p,{children:["Navigate to your ROS2 workspace's ",(0,o.jsx)(n.code,{children:"src"})," directory and create a new package:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 pkg create --build-type ament_python --node-name perception_node simple_pa_loop\nros2 pkg create --build-type ament_python --node-name action_node simple_pa_loop\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-implement-the-perception-node",children:"Step 2: Implement the Perception Node"}),"\n",(0,o.jsxs)(n.p,{children:["Open the file ",(0,o.jsx)(n.code,{children:"simple_pa_loop/perception_node.py"}),' and add the following code. This node will publish a simple "Object detected" message every second.']}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n        self.publisher_ = self.create_publisher(String, 'sensor_data', 10)\n        timer_period = 1.0  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        self.get_logger().info('Perception node started. Publishing sensor data...')\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Object detected'\n        self.publisher_.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = PerceptionNode()\n    rclpy.spin(perception_node)\n    perception_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-implement-the-action-node",children:"Step 3: Implement the Action Node"}),"\n",(0,o.jsxs)(n.p,{children:["Open the file ",(0,o.jsx)(n.code,{children:"simple_pa_loop/action_node.py"})," and add the following code. This node subscribes to the ",(0,o.jsx)(n.code,{children:"sensor_data"})," topic and logs a message when it receives data."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass ActionNode(Node):\n    def __init__(self):\n        super().__init__('action_node')\n        self.subscription = self.create_subscription(\n            String,\n            'sensor_data',\n            self.listener_callback,\n            10)\n        self.subscription  # prevent unused variable warning\n        self.get_logger().info('Action node started. Listening for sensor data...')\n\n    def listener_callback(self, msg):\n        self.get_logger().info('I heard: \"%s\". Taking action: [Stopping Motor]' % msg.data)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    action_node = ActionNode()\n    rclpy.spin(action_node)\n    action_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-build-and-run",children:"Step 4: Build and Run"}),"\n",(0,o.jsx)(n.p,{children:"In your workspace root, build the package:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"colcon build --packages-select simple_pa_loop\n"})}),"\n",(0,o.jsx)(n.p,{children:"Source your workspace:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n",(0,o.jsx)(n.p,{children:"Run the nodes in separate terminals:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 1\nros2 run simple_pa_loop perception_node\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 2\nros2 run simple_pa_loop action_node\n"})}),"\n",(0,o.jsx)(n.p,{children:"You should see the perception node publishing messages and the action node receiving them, simulating a simple Perception-Action loop."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"case-study-shakey-the-robot---the-first-person",children:'Case Study: Shakey the Robot - The First "Person"'}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Introduction:"}),'\nDeveloped at the Stanford Research Institute (SRI) from 1966 to 1972, Shakey was the first mobile robot to reason about its own actions. It was nicknamed "Shakey" because of its wobbly and shaky movements. Despite its physical limitations, Shakey was a landmark in artificial intelligence and robotics, as it was the first project to integrate logical reasoning and physical action.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"The Challenge:"}),"\nThe goal of the Shakey project was to create a robot that could navigate a complex environment, understand commands given in natural language, and formulate and execute plans to achieve goals. This was a radical departure from the purely abstract problem-solving of the time."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Shakey's Architecture - A Hybrid Approach:"}),"\nShakey's software was structured in layers, representing a hybrid deliberative-reactive architecture. This was a precursor to the more sophisticated architectures seen in modern robotics. The layers included:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Low-level actions:"})," Basic motor controls for moving and turning."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intermediate-level actions:"})," Short routines for navigating through doorways or pushing boxes."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High-level reasoning:"})," A planner called STRIPS (Stanford Research Institute Problem Solver) that could generate a sequence of actions to achieve a goal."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"The Perception-Action Loop in Practice:"}),"\nShakey embodied a complete, albeit slow, perception-action loop."]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception:"}),' Shakey used a television camera, a triangulating rangefinder, and "bump sensors" (whiskers) to build a model of its world.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognition/Control:"}),' When given a command like "Go to the next room and push the block off the platform," Shakey\'s STRIPS planner would analyze its internal model of the world and generate a plan. For example: ',(0,o.jsx)(n.code,{children:"[Go to Doorway, Go through Doorway, Go to Platform, Push Block]"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action:"})," Shakey would then execute the plan, one step at a time, using its wheels and a push bar. After each step, it would update its internal model based on new sensor data."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Legacy and Impact:"}),"\nShakey's development led to numerous breakthroughs that are now fundamental to robotics and AI, including the A* search algorithm (for pathfinding) and the Hough transform (for computer vision). More importantly, Shakey demonstrated that a physical agent could bridge the gap between abstract reasoning and real-world action. While it was slow and clumsy, Shakey was a powerful proof of concept for the field of Physical AI, demonstrating that an agent could perceive, plan, and act in an unstructured environment (Nilsson, 1984)."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"references-1",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Nilsson, N. J. (1984). ",(0,o.jsx)(n.em,{children:"Shakey the robot"}),". SRI INTERNATIONAL."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);