"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[253],{2687:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"AI-for-Robotics-From-Learning-to-Reasoning/plan","title":"Chapter Plan: AI for Robotics: From Learning to Reasoning","description":"This document outlines the plan for the chapter \\"AI for Robotics: From Learning to Reasoning\\".","source":"@site/docs/AI-for-Robotics-From-Learning-to-Reasoning/00-plan.md","sourceDirName":"AI-for-Robotics-From-Learning-to-Reasoning","slug":"/AI-for-Robotics-From-Learning-to-Reasoning/plan","permalink":"/textbook.github.io/docs/AI-for-Robotics-From-Learning-to-Reasoning/plan","draft":false,"unlisted":false,"editUrl":"https://github.com/Mohammed-Umair-Kalim/textbook.github.io/docs/AI-for-Robotics-From-Learning-to-Reasoning/00-plan.md","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Tutorial Intro","permalink":"/textbook.github.io/docs/intro"},"next":{"title":"Foundation Models for Robotics","permalink":"/textbook.github.io/docs/AI-for-Robotics-From-Learning-to-Reasoning/Foundation-Models-for-Robotics"}}');var t=i(4848),r=i(8453);const s={},a="Chapter Plan: AI for Robotics: From Learning to Reasoning",l={},c=[{value:"Summary",id:"summary",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Topics",id:"key-topics",level:2},{value:"Required Citations",id:"required-citations",level:2}];function d(n){const e={em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-plan-ai-for-robotics-from-learning-to-reasoning",children:"Chapter Plan: AI for Robotics: From Learning to Reasoning"})}),"\n",(0,t.jsx)(e.p,{children:'This document outlines the plan for the chapter "AI for Robotics: From Learning to Reasoning".'}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This chapter explores the cutting-edge intersection of artificial intelligence and robotics, focusing on how advanced AI techniques are being integrated into physical agents. We will delve into the concept of foundation models specifically designed for robotics, explore the power of world models and model predictive control in enabling intelligent behavior, and discuss how robots can achieve multi-modal reasoning to interpret and interact with the complex real world."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the application and potential of foundation models in robotics."}),"\n",(0,t.jsx)(e.li,{children:"Explain the role of world models in robot learning and predictive control."}),"\n",(0,t.jsx)(e.li,{children:"Describe how multi-modal reasoning enhances a physical agent's understanding of its environment."}),"\n",(0,t.jsx)(e.li,{children:"Discuss the challenges and opportunities of integrating learning and reasoning in robotic systems."}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-topics",children:"Key Topics"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Foundation Models for Robotics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Large language models (LLMs) and large visual models (LVMs) for robot policy generation"}),"\n",(0,t.jsx)(e.li,{children:"Pre-training and fine-tuning for embodied tasks"}),"\n",(0,t.jsx)(e.li,{children:"Challenges in real-world deployment"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"World Models and Predictive Control"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Learning latent representations of the environment"}),"\n",(0,t.jsx)(e.li,{children:"Model-based reinforcement learning"}),"\n",(0,t.jsx)(e.li,{children:"Planning with learned dynamics"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-modal Reasoning for Physical Agents"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrating vision, language, and tactile data"}),"\n",(0,t.jsx)(e.li,{children:"Grounding abstract concepts in physical reality"}),"\n",(0,t.jsx)(e.li,{children:"Human-robot dialogue and instruction following"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"required-citations",children:"Required Citations"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Huang, C., et al. (2023). ",(0,t.jsx)(e.em,{children:"VIMA: An Emergent Universal Agent for Computer Control with Visual, Language, and Action Prompts"}),". arXiv preprint arXiv:2303.01134."]}),"\n",(0,t.jsxs)(e.li,{children:["Hafner, D., et al. (2019). ",(0,t.jsx)(e.em,{children:"Dream to Control: Learning Behaviors by Latent Imagination"}),". ICLR 2020."]}),"\n",(0,t.jsxs)(e.li,{children:["Wen, L., et al. (2022). ",(0,t.jsx)(e.em,{children:"Bridging the Reality Gap with Generative World Models"}),". CoRL 2022."]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(n){const e=o.useContext(r);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),o.createElement(r.Provider,{value:e},n.children)}}}]);